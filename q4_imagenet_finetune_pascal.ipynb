{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4 Shoulders of Giants (15 points)\n",
    "As we have already seen, deep networks can sometimes be hard to optimize. Often times they heavily overfit on small training sets. Many approaches have been proposed to counter this, eg, [Krahenbuhl et al. (ICLRâ€™16)](http://arxiv.org/pdf/1511.06856.pdf), self-supervised learning, etc. However, the most effective approach remains pre-training the network on large, well-labeled supervised datasets such as ImageNet. \n",
    "\n",
    "While training on the full ImageNet data is beyond the scope of this assignment, people have already trained many popular/standard models and released them online. In this task, we will initialize a ResNet-18 model with pre-trained ImageNet weights (from `torchvision`), and finetune the network for PASCAL classification.\n",
    "\n",
    "## 4.1 Load Pre-trained Model (7 pts)\\\n",
    "Load the pre-trained weights up to the second last layer, and initialize last layer from scratch (the very last layer that outputs the classes).\n",
    "\n",
    "The model loading mechanism is based on names of the weights. It is easy to load pretrained models from `torchvision.models`, even when your model uses different names for weights. Please briefly explain how to load the weights correctly if the names do not match ([hint](https://discuss.pytorch.org/t/loading-weights-from-pretrained-model-with-different-module-names/11841)).\n",
    "\n",
    "**YOUR ANSWER HERE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import trainer\n",
    "from utils import ARGS\n",
    "from simple_cnn import SimpleCNN\n",
    "from voc_dataset import VOCDataset\n",
    "\n",
    "\n",
    "# Pre-trained weights up to second-to-last layer\n",
    "# final layers should be initialized from scratch!\n",
    "class PretrainedResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ResNet = models.resnet18(pretrained=True)\n",
    "        in_features = self.ResNet.fc.in_features\n",
    "        self.ResNet.fc = nn.Sequential(nn.Linear(in_features, 20))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.ResNet(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet.conv1.weight\n",
      "ResNet.bn1.weight\n",
      "ResNet.bn1.bias\n",
      "ResNet.bn1.running_mean\n",
      "ResNet.bn1.running_var\n",
      "ResNet.bn1.num_batches_tracked\n",
      "ResNet.layer1.0.conv1.weight\n",
      "ResNet.layer1.0.bn1.weight\n",
      "ResNet.layer1.0.bn1.bias\n",
      "ResNet.layer1.0.bn1.running_mean\n",
      "ResNet.layer1.0.bn1.running_var\n",
      "ResNet.layer1.0.bn1.num_batches_tracked\n",
      "ResNet.layer1.0.conv2.weight\n",
      "ResNet.layer1.0.bn2.weight\n",
      "ResNet.layer1.0.bn2.bias\n",
      "ResNet.layer1.0.bn2.running_mean\n",
      "ResNet.layer1.0.bn2.running_var\n",
      "ResNet.layer1.0.bn2.num_batches_tracked\n",
      "ResNet.layer1.1.conv1.weight\n",
      "ResNet.layer1.1.bn1.weight\n",
      "ResNet.layer1.1.bn1.bias\n",
      "ResNet.layer1.1.bn1.running_mean\n",
      "ResNet.layer1.1.bn1.running_var\n",
      "ResNet.layer1.1.bn1.num_batches_tracked\n",
      "ResNet.layer1.1.conv2.weight\n",
      "ResNet.layer1.1.bn2.weight\n",
      "ResNet.layer1.1.bn2.bias\n",
      "ResNet.layer1.1.bn2.running_mean\n",
      "ResNet.layer1.1.bn2.running_var\n",
      "ResNet.layer1.1.bn2.num_batches_tracked\n",
      "ResNet.layer2.0.conv1.weight\n",
      "ResNet.layer2.0.bn1.weight\n",
      "ResNet.layer2.0.bn1.bias\n",
      "ResNet.layer2.0.bn1.running_mean\n",
      "ResNet.layer2.0.bn1.running_var\n",
      "ResNet.layer2.0.bn1.num_batches_tracked\n",
      "ResNet.layer2.0.conv2.weight\n",
      "ResNet.layer2.0.bn2.weight\n",
      "ResNet.layer2.0.bn2.bias\n",
      "ResNet.layer2.0.bn2.running_mean\n",
      "ResNet.layer2.0.bn2.running_var\n",
      "ResNet.layer2.0.bn2.num_batches_tracked\n",
      "ResNet.layer2.0.downsample.0.weight\n",
      "ResNet.layer2.0.downsample.1.weight\n",
      "ResNet.layer2.0.downsample.1.bias\n",
      "ResNet.layer2.0.downsample.1.running_mean\n",
      "ResNet.layer2.0.downsample.1.running_var\n",
      "ResNet.layer2.0.downsample.1.num_batches_tracked\n",
      "ResNet.layer2.1.conv1.weight\n",
      "ResNet.layer2.1.bn1.weight\n",
      "ResNet.layer2.1.bn1.bias\n",
      "ResNet.layer2.1.bn1.running_mean\n",
      "ResNet.layer2.1.bn1.running_var\n",
      "ResNet.layer2.1.bn1.num_batches_tracked\n",
      "ResNet.layer2.1.conv2.weight\n",
      "ResNet.layer2.1.bn2.weight\n",
      "ResNet.layer2.1.bn2.bias\n",
      "ResNet.layer2.1.bn2.running_mean\n",
      "ResNet.layer2.1.bn2.running_var\n",
      "ResNet.layer2.1.bn2.num_batches_tracked\n",
      "ResNet.layer3.0.conv1.weight\n",
      "ResNet.layer3.0.bn1.weight\n",
      "ResNet.layer3.0.bn1.bias\n",
      "ResNet.layer3.0.bn1.running_mean\n",
      "ResNet.layer3.0.bn1.running_var\n",
      "ResNet.layer3.0.bn1.num_batches_tracked\n",
      "ResNet.layer3.0.conv2.weight\n",
      "ResNet.layer3.0.bn2.weight\n",
      "ResNet.layer3.0.bn2.bias\n",
      "ResNet.layer3.0.bn2.running_mean\n",
      "ResNet.layer3.0.bn2.running_var\n",
      "ResNet.layer3.0.bn2.num_batches_tracked\n",
      "ResNet.layer3.0.downsample.0.weight\n",
      "ResNet.layer3.0.downsample.1.weight\n",
      "ResNet.layer3.0.downsample.1.bias\n",
      "ResNet.layer3.0.downsample.1.running_mean\n",
      "ResNet.layer3.0.downsample.1.running_var\n",
      "ResNet.layer3.0.downsample.1.num_batches_tracked\n",
      "ResNet.layer3.1.conv1.weight\n",
      "ResNet.layer3.1.bn1.weight\n",
      "ResNet.layer3.1.bn1.bias\n",
      "ResNet.layer3.1.bn1.running_mean\n",
      "ResNet.layer3.1.bn1.running_var\n",
      "ResNet.layer3.1.bn1.num_batches_tracked\n",
      "ResNet.layer3.1.conv2.weight\n",
      "ResNet.layer3.1.bn2.weight\n",
      "ResNet.layer3.1.bn2.bias\n",
      "ResNet.layer3.1.bn2.running_mean\n",
      "ResNet.layer3.1.bn2.running_var\n",
      "ResNet.layer3.1.bn2.num_batches_tracked\n",
      "ResNet.layer4.0.conv1.weight\n",
      "ResNet.layer4.0.bn1.weight\n",
      "ResNet.layer4.0.bn1.bias\n",
      "ResNet.layer4.0.bn1.running_mean\n",
      "ResNet.layer4.0.bn1.running_var\n",
      "ResNet.layer4.0.bn1.num_batches_tracked\n",
      "ResNet.layer4.0.conv2.weight\n",
      "ResNet.layer4.0.bn2.weight\n",
      "ResNet.layer4.0.bn2.bias\n",
      "ResNet.layer4.0.bn2.running_mean\n",
      "ResNet.layer4.0.bn2.running_var\n",
      "ResNet.layer4.0.bn2.num_batches_tracked\n",
      "ResNet.layer4.0.downsample.0.weight\n",
      "ResNet.layer4.0.downsample.1.weight\n",
      "ResNet.layer4.0.downsample.1.bias\n",
      "ResNet.layer4.0.downsample.1.running_mean\n",
      "ResNet.layer4.0.downsample.1.running_var\n",
      "ResNet.layer4.0.downsample.1.num_batches_tracked\n",
      "ResNet.layer4.1.conv1.weight\n",
      "ResNet.layer4.1.bn1.weight\n",
      "ResNet.layer4.1.bn1.bias\n",
      "ResNet.layer4.1.bn1.running_mean\n",
      "ResNet.layer4.1.bn1.running_var\n",
      "ResNet.layer4.1.bn1.num_batches_tracked\n",
      "ResNet.layer4.1.conv2.weight\n",
      "ResNet.layer4.1.bn2.weight\n",
      "ResNet.layer4.1.bn2.bias\n",
      "ResNet.layer4.1.bn2.running_mean\n",
      "ResNet.layer4.1.bn2.running_var\n",
      "ResNet.layer4.1.bn2.num_batches_tracked\n",
      "ResNet.fc.0.weight\n",
      "ResNet.fc.0.bias\n"
     ]
    }
   ],
   "source": [
    "model = PretrainedResNet()\n",
    "dicts = model.state_dict()\n",
    "for key,value in dicts.items():\n",
    "  print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model with a similar hyperparameter setup as in the scratch case. No need to freeze the loaded weights. Show the learning curves (training loss, testing MAP) for 10 epochs. Please evaluate your model to calculate the MAP on the testing dataset every 100 iterations. Also feel free to tune the hyperparameters to improve performance.\n",
    "\n",
    "**REMEMBER TO SAVE MODEL AT END OF TRAINING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args.batch_size = 32\n",
      "args.device = cuda\n",
      "args.epochs = 10\n",
      "args.gamma = 0.75\n",
      "args.inp_size = 224\n",
      "args.log_every = 100\n",
      "args.lr = 0.0001\n",
      "args.save_at_end = True\n",
      "args.save_freq = -1\n",
      "args.step_size = 5\n",
      "args.test_batch_size = 128\n",
      "args.val_every = 100\n",
      "\n",
      "Train Epoch: 0 [0 (0%)]\tLoss: 0.684893\n",
      "Train Epoch: 0 [100 (64%)]\tLoss: 0.144258\n",
      "Train Epoch: 1 [200 (27%)]\tLoss: 0.089405\n",
      "Train Epoch: 1 [300 (91%)]\tLoss: 0.085533\n",
      "Train Epoch: 2 [400 (55%)]\tLoss: 0.079343\n",
      "Train Epoch: 3 [500 (18%)]\tLoss: 0.066151\n",
      "Train Epoch: 3 [600 (82%)]\tLoss: 0.043326\n",
      "Train Epoch: 4 [700 (46%)]\tLoss: 0.040772\n",
      "Train Epoch: 5 [800 (10%)]\tLoss: 0.030136\n",
      "Train Epoch: 5 [900 (73%)]\tLoss: 0.041671\n",
      "Train Epoch: 6 [1000 (37%)]\tLoss: 0.033777\n",
      "Train Epoch: 7 [1100 (1%)]\tLoss: 0.027765\n",
      "Train Epoch: 7 [1200 (64%)]\tLoss: 0.032060\n",
      "Train Epoch: 8 [1300 (28%)]\tLoss: 0.016630\n",
      "Train Epoch: 8 [1400 (92%)]\tLoss: 0.019418\n",
      "Train Epoch: 9 [1500 (55%)]\tLoss: 0.012074\n",
      "test map: 0.7560521199568571\n"
     ]
    }
   ],
   "source": [
    "args = ARGS(epochs=10, lr=0.0001, batch_size=32, test_batch_size=128, gamma=0.75, step_size=5, save_at_end=True, save_freq=-1, use_cuda=True)\n",
    "model = PretrainedResNet()\n",
    "print(args)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=args.step_size, gamma=args.gamma)\n",
    "test_ap, test_map = trainer.train(args, model, optimizer, scheduler, model_name='pre-res')\n",
    "print('test map:', test_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR TENSORBOARD SCREENSHOTS HERE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
