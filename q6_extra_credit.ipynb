{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q6: Improve Performance (20 pts)\n",
    "\n",
    "Many techniques have been proposed in the literature to improve classification performance for deep networks. In this section, we try to use a recently proposed technique called [mixup](https://arxiv.org/abs/1710.09412). The main idea is to augment the training set with linear combinations of images and labels. Read through the paper and modify your model to implement mixup. Report your performance, along with training/test curves, and comparison with baseline in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement mixup regularization here and show performance\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "from simple_cnn import SimpleCNN             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ARGS(object):\n",
    "    # input batch size for training \n",
    "    batch_size = 64\n",
    "    # input batch size for testing\n",
    "    test_batch_size=1000\n",
    "    # number of epochs to train for\n",
    "    epochs = 14\n",
    "    # learning rate\n",
    "    lr = 1e-3\n",
    "    # Learning rate step gamma\n",
    "    gamma = 0.7\n",
    "    # how many batches to wait before logging training status\n",
    "    log_every = 100\n",
    "    # how many batches to wait before evaluating model\n",
    "    val_every = 100\n",
    "    # set true if using GPU during training\n",
    "    use_cuda = False\n",
    "\n",
    "args = ARGS()\n",
    "device = torch.device(\"cuda\" if args.use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_data(x, y, alpha=1.0, use_cuda=True):\n",
    "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    if use_cuda:\n",
    "        index = torch.randperm(batch_size).cuda()\n",
    "    else:\n",
    "        index = torch.randperm(batch_size)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    \"\"\"Evaluate model on test dataset.\"\"\"\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "    return test_loss, correct / len(test_loader.dataset)\n",
    "\n",
    "def main():\n",
    "    # 1. load dataset and build dataloader\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.FashionMNIST('../data', train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=args.batch_size, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.FashionMNIST('../data', train=False, transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=args.test_batch_size, shuffle=True)\n",
    "\n",
    "    # 2. define the model, and optimizer.\n",
    "    model = SimpleCNN().to(device)\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=args.gamma)\n",
    "    cnt = 0\n",
    "    train_log = {'iter': [], 'loss': [], 'accuracy': []}\n",
    "    test_log = {'iter': [], 'loss': [], 'accuracy': []}\n",
    "    correct = 0\n",
    "    for epoch in range(args.epochs):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            # Get a batch of data\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            output = model(data)\n",
    "            # Calculate the loss\n",
    "            loss = F.cross_entropy(output, target)\n",
    "            # Calculate gradient w.r.t the loss\n",
    "            loss.backward()\n",
    "            # Optimizer takes one step\n",
    "            optimizer.step()\n",
    "            \n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            train_acc = correct / (args.batch_size*(cnt+1))\n",
    "            \n",
    "            # Log info\n",
    "            if cnt % args.log_every == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, cnt, len(train_loader.dataset),\n",
    "                           100. * batch_idx / len(train_loader), loss.item()))\n",
    "                train_log['iter'].append(cnt)\n",
    "                train_log['loss'].append(loss.cpu().detach().numpy())\n",
    "                # TODO: q0.1 calculate your train accuracy!\n",
    "                train_log['accuracy'].append(train_acc)\n",
    "            # Validation iteration\n",
    "            if cnt % args.val_every == 0:\n",
    "                test_loss, test_acc = test(model, device, test_loader)\n",
    "                test_log['iter'].append(cnt)\n",
    "                test_log['loss'].append(test_loss)\n",
    "                test_log['accuracy'].append(test_acc)\n",
    "                model.train()\n",
    "            cnt += 1\n",
    "        scheduler.step()\n",
    "    fig = plt.figure()\n",
    "    plt.plot(train_log['iter'], train_log['loss'], 'r', label='Training')\n",
    "    plt.plot(test_log['iter'], test_log['loss'], 'b', label='Testing')\n",
    "    plt.title('Loss')\n",
    "    plt.legend()\n",
    "    fig = plt.figure()\n",
    "    plt.plot(train_log['iter'], train_log['accuracy'], 'r', label='Training')\n",
    "    plt.plot(test_log['iter'], test_log['accuracy'], 'b', label='Testing')\n",
    "    plt.title('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
