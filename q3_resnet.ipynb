{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3: Even deeper! Resnet18 for PASCAL classification (15 pts)\n",
    "\n",
    "Hopefully we all got much better accuracy with the deeper model! Since 2012, much deeper architectures have been proposed. [ResNet](https://arxiv.org/abs/1512.03385) is one of the popular ones. In this task, we attempt to further improve the performance with the “very deep” ResNet-18 architecture.\n",
    "\n",
    "\n",
    "## 3.1 Build ResNet-18 (1 pts)\n",
    "Write a network modules for the Resnet-18 architecture (refer to the original paper). You can use `torchvision.models` for this section, so it should be very easy! \n",
    "Do not load the pretrained weights for this question. We will get to that in the next question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import trainer\n",
    "from utils import ARGS\n",
    "from simple_cnn import SimpleCNN\n",
    "from voc_dataset import VOCDataset\n",
    "\n",
    "\n",
    "# you could write the whole class....\n",
    "# or one line :D\n",
    "ResNet = models.resnet18(pretrained=False)\n",
    "# # print(model)\n",
    "# ResNet = torch.nn.Sequential(*(list(model.children())[:-1]))\n",
    "# # print(ResNet)\n",
    "# ResNet.add_module(\"fc\", nn.Linear(512, 20))\n",
    "# print(ResNet)\n",
    "in_features = ResNet.fc.in_features\n",
    "ResNet.fc = nn.Sequential(nn.Linear(in_features, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Add Tensorboard Summaries (6 pts)\n",
    "You should've already written tensorboard summary generation code into `trainer.py` from q1. However, you probably just added the most basic summary features. Please implement the more advanced summaries listed here:\n",
    "* training loss (should be done)\n",
    "* testing MAP curves (should be done)\n",
    "* learning rate\n",
    "* [histogram of gradients](https://www.tensorflow.org/api_docs/python/tf/summary/histogram)\n",
    "\n",
    "## 3.3 Train and Test (8 pts)\n",
    "Use the same hyperparameter settings from Task 2, and train the model for 50 epochs. Tune hyperparameters properly to get mAP around 0.5. Report tensorboard screenshots for *all* of the summaries listed above (for image summaries show screenshots at $n \\geq 3$ iterations). For the histograms, include the screenshots of the gradients of layer1.1.conv1.weight and layer4.0.bn2.bias.\n",
    "\n",
    "**REMEMBER TO SAVE A MODEL AT THE END OF TRAINING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args.batch_size = 32\n",
      "args.device = cuda\n",
      "args.epochs = 50\n",
      "args.gamma = 0.1\n",
      "args.inp_size = 224\n",
      "args.log_every = 100\n",
      "args.lr = 0.0001\n",
      "args.save_at_end = True\n",
      "args.save_freq = 10\n",
      "args.step_size = 30\n",
      "args.test_batch_size = 128\n",
      "args.val_every = 250\n",
      "\n",
      "Train Epoch: 0 [0 (0%)]\tLoss: 0.699209\n",
      "Train Epoch: 0 [100 (64%)]\tLoss: 0.202564\n",
      "Train Epoch: 1 [200 (27%)]\tLoss: 0.222251\n",
      "Train Epoch: 1 [300 (91%)]\tLoss: 0.205923\n",
      "Train Epoch: 2 [400 (55%)]\tLoss: 0.214943\n",
      "Train Epoch: 3 [500 (18%)]\tLoss: 0.189991\n",
      "Train Epoch: 3 [600 (82%)]\tLoss: 0.168802\n",
      "Train Epoch: 4 [700 (46%)]\tLoss: 0.192032\n",
      "Train Epoch: 5 [800 (10%)]\tLoss: 0.191507\n",
      "Train Epoch: 5 [900 (73%)]\tLoss: 0.183262\n",
      "Train Epoch: 6 [1000 (37%)]\tLoss: 0.178853\n",
      "Train Epoch: 7 [1100 (1%)]\tLoss: 0.170648\n",
      "Train Epoch: 7 [1200 (64%)]\tLoss: 0.186747\n",
      "Train Epoch: 8 [1300 (28%)]\tLoss: 0.173882\n",
      "Train Epoch: 8 [1400 (92%)]\tLoss: 0.167580\n",
      "Train Epoch: 9 [1500 (55%)]\tLoss: 0.146841\n",
      "Train Epoch: 10 [1600 (19%)]\tLoss: 0.141675\n",
      "Train Epoch: 10 [1700 (83%)]\tLoss: 0.144456\n",
      "Train Epoch: 11 [1800 (46%)]\tLoss: 0.121146\n",
      "Train Epoch: 12 [1900 (10%)]\tLoss: 0.160855\n",
      "Train Epoch: 12 [2000 (74%)]\tLoss: 0.127704\n",
      "Train Epoch: 13 [2100 (38%)]\tLoss: 0.180495\n",
      "Train Epoch: 14 [2200 (1%)]\tLoss: 0.127173\n",
      "Train Epoch: 14 [2300 (65%)]\tLoss: 0.146744\n",
      "Train Epoch: 15 [2400 (29%)]\tLoss: 0.140973\n",
      "Train Epoch: 15 [2500 (92%)]\tLoss: 0.141927\n",
      "Train Epoch: 16 [2600 (56%)]\tLoss: 0.117352\n",
      "Train Epoch: 17 [2700 (20%)]\tLoss: 0.101802\n",
      "Train Epoch: 17 [2800 (83%)]\tLoss: 0.129283\n",
      "Train Epoch: 18 [2900 (47%)]\tLoss: 0.123639\n",
      "Train Epoch: 19 [3000 (11%)]\tLoss: 0.112150\n",
      "Train Epoch: 19 [3100 (75%)]\tLoss: 0.112695\n",
      "Train Epoch: 20 [3200 (38%)]\tLoss: 0.106540\n",
      "Train Epoch: 21 [3300 (2%)]\tLoss: 0.099987\n",
      "Train Epoch: 21 [3400 (66%)]\tLoss: 0.099110\n",
      "Train Epoch: 22 [3500 (29%)]\tLoss: 0.090712\n",
      "Train Epoch: 22 [3600 (93%)]\tLoss: 0.121629\n",
      "Train Epoch: 23 [3700 (57%)]\tLoss: 0.088478\n",
      "Train Epoch: 24 [3800 (20%)]\tLoss: 0.084751\n",
      "Train Epoch: 24 [3900 (84%)]\tLoss: 0.113569\n",
      "Train Epoch: 25 [4000 (48%)]\tLoss: 0.074464\n",
      "Train Epoch: 26 [4100 (11%)]\tLoss: 0.061779\n",
      "Train Epoch: 26 [4200 (75%)]\tLoss: 0.066068\n",
      "Train Epoch: 27 [4300 (39%)]\tLoss: 0.055811\n",
      "Train Epoch: 28 [4400 (3%)]\tLoss: 0.056462\n",
      "Train Epoch: 28 [4500 (66%)]\tLoss: 0.072026\n",
      "Train Epoch: 29 [4600 (30%)]\tLoss: 0.047537\n",
      "Train Epoch: 29 [4700 (94%)]\tLoss: 0.060105\n",
      "Train Epoch: 30 [4800 (57%)]\tLoss: 0.037097\n",
      "Train Epoch: 31 [4900 (21%)]\tLoss: 0.035616\n",
      "Train Epoch: 31 [5000 (85%)]\tLoss: 0.028001\n",
      "Train Epoch: 32 [5100 (48%)]\tLoss: 0.040750\n",
      "Train Epoch: 33 [5200 (12%)]\tLoss: 0.028263\n",
      "Train Epoch: 33 [5300 (76%)]\tLoss: 0.050090\n",
      "Train Epoch: 34 [5400 (39%)]\tLoss: 0.034551\n",
      "Train Epoch: 35 [5500 (3%)]\tLoss: 0.036718\n",
      "Train Epoch: 35 [5600 (67%)]\tLoss: 0.041752\n",
      "Train Epoch: 36 [5700 (31%)]\tLoss: 0.032925\n",
      "Train Epoch: 36 [5800 (94%)]\tLoss: 0.031059\n",
      "Train Epoch: 37 [5900 (58%)]\tLoss: 0.022926\n",
      "Train Epoch: 38 [6000 (22%)]\tLoss: 0.037844\n",
      "Train Epoch: 38 [6100 (85%)]\tLoss: 0.035111\n",
      "Train Epoch: 39 [6200 (49%)]\tLoss: 0.027122\n",
      "Train Epoch: 40 [6300 (13%)]\tLoss: 0.029809\n",
      "Train Epoch: 40 [6400 (76%)]\tLoss: 0.025342\n",
      "Train Epoch: 41 [6500 (40%)]\tLoss: 0.027599\n",
      "Train Epoch: 42 [6600 (4%)]\tLoss: 0.027147\n",
      "Train Epoch: 42 [6700 (68%)]\tLoss: 0.034832\n",
      "Train Epoch: 43 [6800 (31%)]\tLoss: 0.035893\n",
      "Train Epoch: 43 [6900 (95%)]\tLoss: 0.037944\n",
      "Train Epoch: 44 [7000 (59%)]\tLoss: 0.026192\n",
      "Train Epoch: 45 [7100 (22%)]\tLoss: 0.028801\n",
      "Train Epoch: 45 [7200 (86%)]\tLoss: 0.040125\n",
      "Train Epoch: 46 [7300 (50%)]\tLoss: 0.028890\n",
      "Train Epoch: 47 [7400 (13%)]\tLoss: 0.024793\n",
      "Train Epoch: 47 [7500 (77%)]\tLoss: 0.025705\n",
      "Train Epoch: 48 [7600 (41%)]\tLoss: 0.043020\n",
      "Train Epoch: 49 [7700 (4%)]\tLoss: 0.023454\n",
      "Train Epoch: 49 [7800 (68%)]\tLoss: 0.015499\n",
      "test map: 0.5025014117872961\n"
     ]
    }
   ],
   "source": [
    "args = ARGS(epochs=50, lr=0.0001, batch_size=32, save_at_end=True, save_freq=10, step_size=30, test_batch_size=128, use_cuda=True, gamma=0.1, val_every=250)\n",
    "resnet = ResNet\n",
    "print(args)\n",
    "optimizer = torch.optim.Adam(resnet.parameters(), lr=args.lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=args.step_size, gamma=args.gamma)\n",
    "test_ap, test_map = trainer.train(args, resnet, optimizer, scheduler, model_name='resnet2')\n",
    "print('test map:', test_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Loss for training***\n",
    "\n",
    "<img src=\"vlr-hw1-images/q3-loss.png\"/>\n",
    "\n",
    "\n",
    "***mAP for testing*** \n",
    "\n",
    "<img src=\"vlr-hw1-images/q3-map.png\"/>\n",
    "\n",
    "***Learning Rate*** \n",
    "\n",
    "<img src=\"vlr-hw1-images/q3-lr.png\"/>\n",
    "\n",
    "\n",
    "***Histogram layer1.1.conv1.weight***\n",
    "\n",
    "<img src=\"vlr-hw1-images/q3-hist-conv1.png\"/>\n",
    "\n",
    "\n",
    "***Histogram layer4.0.bn2.bias*** \n",
    "\n",
    "<img src=\"vlr-hw1-images/q3-hist-bias.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
